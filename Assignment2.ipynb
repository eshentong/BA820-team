{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89806e83-83ba-4dfe-8dfa-59d99908076b",
   "metadata": {
    "id": "89806e83-83ba-4dfe-8dfa-59d99908076b",
    "tags": []
   },
   "source": [
    "# Description of Data\n",
    "\n",
    "The AI-GA (Artificial Intelligence Generated Abstracts) dataset is a collection of paper abstracts, either AI-generated or original.\n",
    "\n",
    "The AI-generated abstracts are generated using state-of-the-art language generation techniques (GPT-3 model).\n",
    "\n",
    "The dataset is provided in CSV format, with each row representing a single sample (i.e.,  a single abstract).\n",
    "\n",
    "*The ultimate goal of this assignment is to classify the abstracts based on the source (i.e., whether it is AI-generated or original).*\n",
    "\n",
    "Total sample size: 14,331 (7,248 AI-generated and 7,082 original)\n",
    "\n",
    "Each sample contains three columns: abstract, title, and label. The label indicates whether the sample is an original abstract (labeled as 0) or an AI-generated abstract (labeled as 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BLCw374cLy-p",
   "metadata": {
    "id": "BLCw374cLy-p"
   },
   "source": [
    "##Package installs and imports\n",
    "\n",
    "DO NOT CHANGE THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6185d8c7-3e77-4c7d-8999-6b3f429cdfa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6185d8c7-3e77-4c7d-8999-6b3f429cdfa4",
    "outputId": "ef55b5b2-0137-4493-bf22-defcdbdb3922",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (3.7.4)\n",
      "Requirement already satisfied: click in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d09059c-ecd1-4167-b936-2ca2c8f31350",
   "metadata": {
    "id": "9d09059c-ecd1-4167-b936-2ca2c8f31350",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UThQWFjYaYw6",
   "metadata": {
    "id": "UThQWFjYaYw6"
   },
   "source": [
    "Load dataset **\"ai-ga-dataset.csv\"** as a csv file and save it as a dataframe named **\"abstracts_df\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b869f345-f1ef-4a65-8346-fc8bdff84150",
   "metadata": {
    "id": "b869f345-f1ef-4a65-8346-fc8bdff84150"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Exaggerated Autophagy in Stanford Type A Aorti...</td>\n",
       "      <td>\\n\\nThis study presents a novel transcriptome ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ABO blood types and sepsis mortality</td>\n",
       "      <td>\\n\\nThe ABO blood types have been associated w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AAV8-Mediated Angiotensin-Converting Enzyme 2 ...</td>\n",
       "      <td>\\n\\nTitle: AAV8-Mediated Angiotensin-Convertin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MyCare study: protocol for a controlled trial ...</td>\n",
       "      <td>INTRODUCTION: People with serious mental illne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Exploring collective emotion transmission in f...</td>\n",
       "      <td>Collective emotion is the synchronous converge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title  \\\n",
       "0       1  Exaggerated Autophagy in Stanford Type A Aorti...   \n",
       "1       2               ABO blood types and sepsis mortality   \n",
       "2       3  AAV8-Mediated Angiotensin-Converting Enzyme 2 ...   \n",
       "3       4  MyCare study: protocol for a controlled trial ...   \n",
       "4       5  Exploring collective emotion transmission in f...   \n",
       "\n",
       "                                            abstract  label  \n",
       "0  \\n\\nThis study presents a novel transcriptome ...      1  \n",
       "1  \\n\\nThe ABO blood types have been associated w...      1  \n",
       "2  \\n\\nTitle: AAV8-Mediated Angiotensin-Convertin...      1  \n",
       "3  INTRODUCTION: People with serious mental illne...      0  \n",
       "4  Collective emotion is the synchronous converge...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df = pd.read_csv(\"https://raw.githubusercontent.com/elhamod/BA820/main/Assignment/Assignment2/ai-ga-dataset.csv\")\n",
    "abstracts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qg8N6QYTeVYd",
   "metadata": {
    "id": "Qg8N6QYTeVYd"
   },
   "source": [
    "##Inspection:\n",
    "\n",
    "**Maximum marks: 5**\n",
    "\n",
    "- Print the number of abstracts that are human or AI generated, respectively.\n",
    "- Check if any abstracts have invalid values. Address them appropriately.\n",
    "- Check if any labels have invalid values. Address them appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ABjNKx1Bh0ZA",
   "metadata": {
    "id": "ABjNKx1Bh0ZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14330 entries, 0 to 14329\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   doc_id    14330 non-null  int64 \n",
      " 1   title     14330 non-null  object\n",
      " 2   abstract  14330 non-null  object\n",
      " 3   label     14330 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 447.9+ KB\n"
     ]
    }
   ],
   "source": [
    "abstracts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Chg-fSLcGPqG",
   "metadata": {
    "id": "Chg-fSLcGPqG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14330, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4194f01-f166-4ea0-a2e2-ff089e90bed9",
   "metadata": {
    "id": "c4194f01-f166-4ea0-a2e2-ff089e90bed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7248 numbers of abtracts were generated by AI, 7082 were generated by human\n"
     ]
    }
   ],
   "source": [
    "ai_gen= abstracts_df[abstracts_df['label']==1]\n",
    "ori_gen= abstracts_df[abstracts_df['label']==0]\n",
    "print(len(ai_gen),\"numbers of abtracts were generated by AI,\", len(ori_gen),\"were generated by human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pmwxgHGfkBk8",
   "metadata": {
    "id": "pmwxgHGfkBk8"
   },
   "source": [
    "- checking for null value of `abstract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mBXLxR1Keym1",
   "metadata": {
    "id": "mBXLxR1Keym1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_gen['abstract'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "_G5sZ77sj9uq",
   "metadata": {
    "id": "_G5sZ77sj9uq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_gen['abstract'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zW7QnVdysYvQ",
   "metadata": {
    "id": "zW7QnVdysYvQ"
   },
   "source": [
    "**Answer**:\n",
    "\n",
    "There doesn't seem to be any invalid values of `abstract` for both AI and human generated contents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b0f12f-453e-4e1d-bdda-6da7ffa7f88b",
   "metadata": {
    "id": "48b0f12f-453e-4e1d-bdda-6da7ffa7f88b"
   },
   "source": [
    "#Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lsKMgZblrttY",
   "metadata": {
    "id": "lsKMgZblrttY"
   },
   "source": [
    "## Question 1.1: text cleaning\n",
    "\n",
    "**Maximum marks: 5**\n",
    "\n",
    "Perform pre-processing on all abstracts by lower-casing and removing all non-alpha-numeric characters (i.e., only keep numbers, English alphabet letters, and white spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9zF1tZK2--VA",
   "metadata": {
    "id": "9zF1tZK2--VA"
   },
   "outputs": [],
   "source": [
    "abstract= abstracts_df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96a6872c-dffc-4e58-9f74-d1c8f566f85d",
   "metadata": {
    "id": "96a6872c-dffc-4e58-9f74-d1c8f566f85d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "#lower casing the text\n",
    "clean_df= abstract.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3689d1b9-7266-4967-89cb-72709ea01173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \\n\\nthis study presents a novel transcriptome ...\n",
       "1        \\n\\nthe abo blood types have been associated w...\n",
       "2        \\n\\ntitle: aav8-mediated angiotensin-convertin...\n",
       "3        introduction: people with serious mental illne...\n",
       "4        collective emotion is the synchronous converge...\n",
       "                               ...                        \n",
       "14325    background: falls are a significant source of ...\n",
       "14326    autosomal dominant polycystic kidney disease (...\n",
       "14327    we study numerically how the structures of dis...\n",
       "14328    \\n\\nthis paper aims to elucidate the role of p...\n",
       "14329    infectious disease threat events (idtes) are i...\n",
       "Name: abstract, Length: 14330, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8F7OxpENoWjx",
   "metadata": {
    "id": "8F7OxpENoWjx"
   },
   "outputs": [],
   "source": [
    "#special characters\n",
    "to_remove = [\"\\W\", \"\\s\"]\n",
    "clean_df= clean_df.replace(to_remove, \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "OcQaHIxcSKUx",
   "metadata": {
    "id": "OcQaHIxcSKUx"
   },
   "outputs": [],
   "source": [
    "clean_df= clean_df.replace('[^\\x00-\\x7F]', \" \", regex= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lVSVzNrzuxXY",
   "metadata": {
    "id": "lVSVzNrzuxXY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this study presents a novel transcriptome pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the abo blood types have been associated wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>title  aav8 mediated angiotensin converting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>introduction  people with serious mental illne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>collective emotion is the synchronous converge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14325</th>\n",
       "      <td>background  falls are a significant source of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14326</th>\n",
       "      <td>autosomal dominant polycystic kidney disease  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14327</th>\n",
       "      <td>we study numerically how the structures of dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14328</th>\n",
       "      <td>this paper aims to elucidate the role of pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14329</th>\n",
       "      <td>infectious disease threat events  idtes  are i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14330 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract\n",
       "0        this study presents a novel transcriptome pi...\n",
       "1        the abo blood types have been associated wit...\n",
       "2        title  aav8 mediated angiotensin converting ...\n",
       "3      introduction  people with serious mental illne...\n",
       "4      collective emotion is the synchronous converge...\n",
       "...                                                  ...\n",
       "14325  background  falls are a significant source of ...\n",
       "14326  autosomal dominant polycystic kidney disease  ...\n",
       "14327  we study numerically how the structures of dis...\n",
       "14328    this paper aims to elucidate the role of pho...\n",
       "14329  infectious disease threat events  idtes  are i...\n",
       "\n",
       "[14330 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9762a58-4b2c-4b85-90e8-7b7db76cb402",
   "metadata": {
    "id": "f9762a58-4b2c-4b85-90e8-7b7db76cb402"
   },
   "source": [
    "## Question 1.2: Stemming or Lemmatization\n",
    "\n",
    "**Maximum Marks: 7.5**\n",
    "\n",
    "We enhance the effectiveness of our text analysis algorithms by normalizing words and reducing them to their root/base forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1b5b77-b4e6-4238-9ce3-dbb0ca677e45",
   "metadata": {
    "id": "3e1b5b77-b4e6-4238-9ce3-dbb0ca677e45",
    "tags": []
   },
   "source": [
    "Write a function `process_text` that\n",
    "\n",
    "\n",
    "\n",
    "1.   removes `english` stop words.\n",
    "2.   uses `PorterStemmer` and `WordNetLemmatizer` to stem AND lemmatize the tokenized abstracts.\n",
    "\n",
    "The function would take in a document and return its tokenization as a list of tokens.\n",
    "\n",
    "To verify its functionality, call the function with the first abstract as input, and then print the transformed abstract as a full text (i.e., as a string, not as a list of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mPau70ORrhuA",
   "metadata": {
    "id": "mPau70ORrhuA"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1df92e-9241-422d-b858-f941d5759f71",
   "metadata": {
    "id": "cc1df92e-9241-422d-b858-f941d5759f71",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    processed_sentences = []\n",
    "    tokenized_text = [word_tokenize(t) for t in clean_df]\n",
    "    for sent in tokenized_text:\n",
    "        text_stemmer= [stemmer.stem(word) for word in sent if word not in stop_words]\n",
    "        text_lemmatizer = [lemmatizer.lemmatize(word) for word in text_stemmer]\n",
    "        processed_sentences.append(text_lemmatizer)\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hk8TeLxQwbsi",
   "metadata": {
    "id": "hk8TeLxQwbsi"
   },
   "outputs": [],
   "source": [
    "processed_text= process_text(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35bb30db-bdd0-442d-871c-7579d542ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts_str = [' '.join(doc) for doc in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f36a46-d324-4944-ac5d-42c820da0f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from more_itertools import flatten\n",
    "# flattened_list = list(flatten(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fdd8229-db76-4243-9998-a248fc5a2b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# sample_text = random.sample(flattened_list, 60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f97db-b178-4fb8-a6c9-ad76d4bc20bb",
   "metadata": {
    "id": "744f97db-b178-4fb8-a6c9-ad76d4bc20bb"
   },
   "source": [
    "#Vectorization\n",
    "\n",
    "Next, we will try different vector representations and see how well each performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb4cc1-8b64-46a9-a49a-575be4850f83",
   "metadata": {
    "id": "0ecb4cc1-8b64-46a9-a49a-575be4850f83"
   },
   "source": [
    "## Question 2.1: Bag of Words\n",
    "\n",
    "**Maximum Marks: 5**\n",
    "\n",
    "Perform Bag of Words on the abstracts and store the vector representation as a DataFrame.\n",
    "\n",
    "You are expected to apply the `process_text` tokenization.\n",
    "\n",
    "Print the head of the resulting DataFrame.\n",
    "\n",
    "How many tokens does BoW yield?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mAjXmFBDsgO6",
   "metadata": {
    "id": "mAjXmFBDsgO6"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c302596-7e24-4b3a-b5c5-a85680e3ed23",
   "metadata": {
    "id": "9c302596-7e24-4b3a-b5c5-a85680e3ed23",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gzHnCi1MP4wx",
   "metadata": {
    "id": "gzHnCi1MP4wx"
   },
   "outputs": [],
   "source": [
    "cv= CountVectorizer()\n",
    "cv_text= cv.fit_transform(processed_texts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "936c7afe-1ee0-4fa1-8cd5-b2762131c0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_df= pd.DataFrame(cv_text.toarray(), columns= cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "w92KBPghRw1b",
   "metadata": {
    "id": "w92KBPghRw1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14330, 45128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25ab45-3680-486c-91cb-e775ccecc2af",
   "metadata": {
    "id": "9e25ab45-3680-486c-91cb-e775ccecc2af"
   },
   "source": [
    "## Question 2.2: TF-IDF\n",
    "\n",
    "**Maximum Marks: 5**\n",
    "\n",
    "Using TF-IDF with `process_text` tokenization, vectorize the abstracts. Then, find the top 5 most similar abstracts to the document with doc_id=6 (shown below) in terms of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "X5x_10MKVgzk",
   "metadata": {
    "id": "X5x_10MKVgzk"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tNAxd5HdVs9v",
   "metadata": {
    "id": "tNAxd5HdVs9v"
   },
   "outputs": [],
   "source": [
    "tf= TfidfVectorizer(norm=None)\n",
    "tf.fit(processed_texts_str)\n",
    "tf_output= tf.transform(processed_texts_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50aa6e94-f26a-4a69-baaa-3ec0650836f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14330, 45128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df= pd.DataFrame(tf_output.toarray(), columns=tf.get_feature_names_out())\n",
    "tf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867017d-aa3d-426b-99f8-1b5cd460db5e",
   "metadata": {},
   "source": [
    "- Cleaning the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4694faa0-55c3-4578-a333-c489a6ea8f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_index = 6\n",
    "index6 = abstracts_df[\"abstract\"].iloc[query_index].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b0d88cd-7545-4e97-a2c4-403ce6acb005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_index6= word_tokenize(index6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac88b28b-6d92-45c3-afb8-2bc475c4769e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_ind6 = tf.transform(processed_index6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "104a9e9b-8e6c-4422-9efa-fca551eb6b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<377x45128 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 156 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_ind6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8102cd74-867f-4597-bc72-d56d50d927ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_df= pd.DataFrame(cosine_similarity(transform_ind6, tf_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ee9d51b-5f4d-4bf3-b137-8ceb9487578b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14320</th>\n",
       "      <th>14321</th>\n",
       "      <th>14322</th>\n",
       "      <th>14323</th>\n",
       "      <th>14324</th>\n",
       "      <th>14325</th>\n",
       "      <th>14326</th>\n",
       "      <th>14327</th>\n",
       "      <th>14328</th>\n",
       "      <th>14329</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>377 rows × 14330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1      2      3      4      5         6      7      8      9      \\\n",
       "0      0.0    0.0    0.0    0.0    0.0    0.0  0.024647    0.0    0.0    0.0   \n",
       "1      0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "2      0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3      0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "4      0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "..     ...    ...    ...    ...    ...    ...       ...    ...    ...    ...   \n",
       "372    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "373    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "374    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "375    0.0    0.0    0.0    0.0    0.0    0.0  0.119376    0.0    0.0    0.0   \n",
       "376    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "\n",
       "     ...  14320     14321  14322  14323  14324     14325  14326  14327  14328  \\\n",
       "0    ...    0.0  0.000000    0.0    0.0    0.0  0.039428    0.0    0.0    0.0   \n",
       "1    ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "2    ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3    ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "4    ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "..   ...    ...       ...    ...    ...    ...       ...    ...    ...    ...   \n",
       "372  ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "373  ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "374  ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "375  ...    0.0  0.013724    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "376  ...    0.0  0.000000    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "\n",
       "     14329  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "..     ...  \n",
       "372    0.0  \n",
       "373    0.0  \n",
       "374    0.0  \n",
       "375    0.0  \n",
       "376    0.0  \n",
       "\n",
       "[377 rows x 14330 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46af2c7f-9d4c-42b1-b406-c2f7fcb0141c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8270     4.243555\n",
       "8629     3.847517\n",
       "1545     3.562759\n",
       "8676     3.168066\n",
       "13061    3.020051\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities= cos_df.sum()\n",
    "similarities_5= (similarities).sort_values(ascending=False)[1:6]\n",
    "similarities_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6320bb5d-f547-40e9-b2b5-ad515d7b7fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in similarities_5.index:\n",
    "#     print(abstracts_df[\"abstract\"].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70d532-dc04-4da5-be50-a2ec38dcec94",
   "metadata": {
    "id": "6f70d532-dc04-4da5-be50-a2ec38dcec94"
   },
   "source": [
    "## Question 2.3 Word2Vec\n",
    "\n",
    "**Maximum Marks: 7.5**\n",
    "\n",
    "Now repeat Q 2.2 but using Word2Vec. For each token, the model should consider the two adjacent tokens on its left and the two on its right. Use a `workers=4` as a parameter to speed up computations. Include **all** possible words that occur in the abstracts.\n",
    "\n",
    "Use vector averaging to calculate the vector representation of the sentence based on the vectors of its constituent words.\n",
    "\n",
    "How do the results of Word2Vec and TF-IDF compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c677c89-0ab4-408d-8e51-e20070e943c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/envs/tensorflow/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "817fd690-3499-46a9-8fbe-9f4c9cf91199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55e2b1a7-f23d-44fe-be3e-8abf47dac1a5",
   "metadata": {
    "id": "55e2b1a7-f23d-44fe-be3e-8abf47dac1a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f32af26c-2b9a-4f6c-8156-fa40595f6020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w2v_model= Word2Vec(sentences= processed_texts_str, vector_size= 20, window=4, min_count=1, workers=4, negative=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "240b7ac7-782b-4d7a-b6f3-6cf3d1b3e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_embeddings = np.array([w2v_model.wv[token] for sentence in processed_texts_str for token in sentence if token in w2v_model.wv.key_to_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "113c39e7-5d5e-408a-9f07-86012822dce2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1908884, 20)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word embedding\n",
    "dataset_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b39db33-a2c2-4613-9ee4-2e08a0b4107e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = []\n",
    "\n",
    "# Iterate over each sentence in processed_text_str\n",
    "for sentence in processed_texts_str:\n",
    "    # Initialize an empty list to store token embeddings\n",
    "    token_embeddings = []\n",
    "    # Iterate over each token in the sentence\n",
    "    for token in sentence:\n",
    "        # Check if the token exists in the Word2Vec model's vocabulary\n",
    "        if token in w2v_model.wv.key_to_index:\n",
    "            # Get the Word2Vec embedding vector for the token\n",
    "            embedding_vector = w2v_model.wv[token]\n",
    "            # Append the embedding vector to the list of token embeddings\n",
    "            token_embeddings.append(embedding_vector)\n",
    "    # Calculate the mean of token embeddings for the current sentence\n",
    "    if token_embeddings:\n",
    "        sentence_embedding = np.mean(token_embeddings, axis=0)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "# Convert the list of sentence embeddings into a NumPy array\n",
    "sentence_embeddings = np.array(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d199353-eb82-4a20-a9d4-5d78a1e73024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14330, 20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a369d4-22bb-4e4a-9086-af3d672e5f62",
   "metadata": {},
   "source": [
    "- Sentence id6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb7083a5-1bfe-4bde-89e4-d1f2b134aeb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_ind6 = np.array([w2v_model.wv[word] for word in processed_index6 if word in w2v_model.wv.key_to_index])\n",
    "embeddings_ind6.shape #sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be91154e-9a6a-4a78-a454-fe8df3f1f726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ind6_embedding = np.mean(embeddings_ind6, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f943718-a914-4cf4-bfb7-b5e2e227637d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind6_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ba622542-c0b3-4b1c-86d9-4b46ea179985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_sim_w2v = cosine_similarity([ind6_embedding], sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cae19b52-d40c-4f06-9f3a-43a93a14a958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7363782 , 0.7387619 , 0.74003905, ..., 0.72811747, 0.7254579 ,\n",
       "        0.72315717]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4cd81489-7d81-47d7-8c8b-cdb7d9f0fd41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6294  7231 11857  3069  4690]\n"
     ]
    }
   ],
   "source": [
    "similarities_5 = cos_sim_glove.argsort()[0][-5:]\n",
    "print(similarities_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pMF3oiDrRZyv",
   "metadata": {
    "id": "pMF3oiDrRZyv"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FS1ASeTTyhGm",
   "metadata": {
    "id": "FS1ASeTTyhGm"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I4vxyXXx7PkT",
   "metadata": {
    "id": "I4vxyXXx7PkT"
   },
   "source": [
    "## Question 3.1: GloVe\n",
    "\n",
    "**Maximum Marks: 7.5**\n",
    "\n",
    "Instead of training our own Word2Vec model, we decided to use a [GloVe](https://nlp.stanford.edu/projects/glove/) model that was pre-trained by researchers at Stanford University. They used a much larger amount of text in their training (e.g., Wikipedia).\n",
    "\n",
    "For this question, simply use `get_tokens(doc)` below for tokenization.\n",
    "\n",
    "**Note:** *Vectorizing the entire dataset using GloVe may take 5-10 minutes. Use the guidelines we discussed in class to test and develop your code before fully applying it to the entire dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "rBXvdoGiCBX2",
   "metadata": {
    "id": "rBXvdoGiCBX2"
   },
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "\n",
    "glove_model = downloader.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ZBgN7-52bzDy",
   "metadata": {
    "id": "ZBgN7-52bzDy"
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# def get_tokens(doc):\n",
    "#     doc_tokenized = nlp(doc)\n",
    "#     tokens = [token.text for token in doc_tokenized]\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a4ffa17-157e-4e76-a5ea-bf4b0bcd7622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# doc_embeddings = []\n",
    "# for sentence in processed_texts_str:\n",
    "#     words = word_tokenize(sentence)\n",
    "#     for word in words:\n",
    "#         if word in glove_model.key_to_index:\n",
    "#             embedding_vector = glove_model.get_vector(word)\n",
    "#             doc_embeddings.append(embedding_vector)\n",
    "# len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e3792403-7a92-4efb-aea5-53e8219ef43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_embeddings = []\n",
    "\n",
    "for sentence in processed_texts_str:\n",
    "    token_embeddings = []\n",
    "    for token in sentence:\n",
    "        if token in glove_model.key_to_index:\n",
    "            embedding_vector = glove_model.get_vector(token)\n",
    "            token_embeddings.append(embedding_vector)\n",
    "    # Calculate the mean of token embeddings for the current sentence\n",
    "    if token_embeddings:\n",
    "        sentence_embedding = np.mean(token_embeddings, axis=0)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "sentence_embeddings = np.array(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c07bd73b-37c9-4cc6-9666-d9bf59f80b38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14330, 50)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8eb52-6979-4065-8016-1538631f3e47",
   "metadata": {},
   "source": [
    "- Sentence id6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f9fbb35a-c2ea-409c-8715-80d244e08a74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(361, 50)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_ind6 = np.array([glove_model.get_vector(word) for word in processed_index6 if word in glove_model.key_to_index])\n",
    "embeddings_ind6.shape #sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "562658f6-17e3-4641-9184-89025bfabbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind6_embedding = np.mean(embeddings_ind6, axis=0)\n",
    "ind6_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420497d9-ffe9-418c-b9b3-c94b48616d95",
   "metadata": {},
   "source": [
    "- Cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c77e7d89-5dbf-4080-ba6c-5e59b973243e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7363782 , 0.7387619 , 0.74003905, ..., 0.72811747, 0.7254579 ,\n",
       "        0.72315717]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim_glove = cosine_similarity([ind6_embedding], sentence_embeddings)\n",
    "cos_sim_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "27088730-913b-4768-a267-62b5b0a67d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6294  7231 11857  3069  4690]\n"
     ]
    }
   ],
   "source": [
    "similarities_5 = cos_sim_glove.argsort()[0][-5:]\n",
    "print(similarities_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xDqkr-icFHBk",
   "metadata": {
    "id": "xDqkr-icFHBk"
   },
   "source": [
    "## Question 3.2: Random Forest Classifier\n",
    "\n",
    "**Maximum Marks: 7.5**\n",
    "\n",
    "Using a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), compare the classification results using GloVe to those using TF-IDF. Does GloVe do better or worse? Are there any particular issues you faced? Elaborate on your findings and justify them.\n",
    "\n",
    "Use a test set of 20% the total dataset size. Use `random_state = 42`.\n",
    "\n",
    "Print the `classification_report` of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R0g9sHVPFptP",
   "metadata": {
    "id": "R0g9sHVPFptP"
   },
   "source": [
    "**Answer**:\n",
    "\n",
    "This shows that TF-IDF is better at predicting the label, as the confusion matrix shows higher TP and TN numbers. This is not surprising as the TF model was trained using this specific dataset, which might've cause some leakage and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "hlSLacl4G1DU",
   "metadata": {
    "id": "hlSLacl4G1DU"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1cd090d4-442b-4856-87fa-17f20c2e4bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Exaggerated Autophagy in Stanford Type A Aorti...</td>\n",
       "      <td>\\n\\nThis study presents a novel transcriptome ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ABO blood types and sepsis mortality</td>\n",
       "      <td>\\n\\nThe ABO blood types have been associated w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AAV8-Mediated Angiotensin-Converting Enzyme 2 ...</td>\n",
       "      <td>\\n\\nTitle: AAV8-Mediated Angiotensin-Convertin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>MyCare study: protocol for a controlled trial ...</td>\n",
       "      <td>INTRODUCTION: People with serious mental illne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Exploring collective emotion transmission in f...</td>\n",
       "      <td>Collective emotion is the synchronous converge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title  \\\n",
       "0       1  Exaggerated Autophagy in Stanford Type A Aorti...   \n",
       "1       2               ABO blood types and sepsis mortality   \n",
       "2       3  AAV8-Mediated Angiotensin-Converting Enzyme 2 ...   \n",
       "3       4  MyCare study: protocol for a controlled trial ...   \n",
       "4       5  Exploring collective emotion transmission in f...   \n",
       "\n",
       "                                            abstract  label  \n",
       "0  \\n\\nThis study presents a novel transcriptome ...      1  \n",
       "1  \\n\\nThe ABO blood types have been associated w...      1  \n",
       "2  \\n\\nTitle: AAV8-Mediated Angiotensin-Convertin...      1  \n",
       "3  INTRODUCTION: People with serious mental illne...      0  \n",
       "4  Collective emotion is the synchronous converge...      0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8cfdbd0f-c88d-48e5-8d48-61a29c8bdf49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X= abstracts_df['abstract'].str.lower()\n",
    "y= abstracts_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e628d95b-bd6d-431f-b212-b71a36b80f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4cf385-7348-4a07-8954-dfce003160fb",
   "metadata": {},
   "source": [
    "- GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "725d6e82-00f9-4de6-85b8-9594fccaefb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_gv=[]\n",
    "\n",
    "for sentence in X_train:\n",
    "    token_embeddings = []\n",
    "    for token in sentence:\n",
    "        if token in glove_model.key_to_index:\n",
    "            embedding_vector = glove_model.get_vector(token)\n",
    "            token_embeddings.append(embedding_vector)\n",
    "    # Calculate the mean of token embeddings for the current sentence\n",
    "    if token_embeddings:\n",
    "        X_train1 = np.mean(token_embeddings, axis=0)\n",
    "        X_train_gv.append(X_train1)\n",
    "\n",
    "X_train_gv = np.array(X_train_gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d923ec7a-0169-4f90-a5bd-5af47d638def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_gv=[]\n",
    "\n",
    "for sentence in X_test:\n",
    "    token_embeddings = []\n",
    "    for token in sentence:\n",
    "        if token in glove_model.key_to_index:\n",
    "            embedding_vector = glove_model.get_vector(token)\n",
    "            token_embeddings.append(embedding_vector)\n",
    "    # Calculate the mean of token embeddings for the current sentence\n",
    "    if token_embeddings:\n",
    "        X_test1 = np.mean(token_embeddings, axis=0)\n",
    "        X_test_gv.append(X_test1)\n",
    "\n",
    "X_test_gv = np.array(X_test_gv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f6cec4d9-8a1f-410c-b745-1407b505038b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1210  216]\n",
      " [ 192 1248]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "rf= RandomForestClassifier()\n",
    "rf.fit(X_train_gv, y_train)\n",
    "y_pred= rf.predict(X_test_gv)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0c62d38d-2a0d-4c31-8748-c1f320df0d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.86      0.85      0.86      1426\\n           1       0.85      0.87      0.86      1440\\n\\n    accuracy                           0.86      2866\\n   macro avg       0.86      0.86      0.86      2866\\nweighted avg       0.86      0.86      0.86      2866\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14c759-8241-433a-9d87-ea5a48281301",
   "metadata": {},
   "source": [
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3202dc86-52fc-4636-8209-806e3981f3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tf= tf.transform(X_train)\n",
    "X_test_tf= tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c7e1c406-d749-41ab-843f-201abcfc2221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1358   68]\n",
      " [  21 1419]]\n"
     ]
    }
   ],
   "source": [
    "rf= RandomForestClassifier()\n",
    "rf.fit(X_train_tf, y_train)\n",
    "y_pred= rf.predict(X_test_tf)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "096e4e4e-62a5-405c-96fa-fcefdb706f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.98      0.95      0.97      1426\\n           1       0.95      0.99      0.97      1440\\n\\n    accuracy                           0.97      2866\\n   macro avg       0.97      0.97      0.97      2866\\nweighted avg       0.97      0.97      0.97      2866\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BLCw374cLy-p"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-env-tensorflow-tensorflow",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "TensorFlow 2-11 (Local)",
   "language": "python",
   "name": "conda-env-tensorflow-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
